{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "from config import ModelArgs\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义softmax函数\n",
    "def softmax(X):\n",
    "    exp_x = np.exp(X - np.max(X, axis=-1, keepdims=True))\n",
    "    return exp_x/np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# 使用Numpy实现的silu函数（即Swish激活函数）\n",
    "def silu(x):\n",
    "    # 计算 sigmod(x)\n",
    "    sigmod_x = 1 / (1 + np.exp(-x))\n",
    "    # 计算 SiLU(x) = x* sigmoid(x)\n",
    "    return x * sigmod_x\n",
    "def silu_gpu(x):\n",
    "    # 计算 sigmod(x)\n",
    "    sigmod_x = 1 / (1 + cp.exp(-x))\n",
    "    # 计算 SiLU(x) = x* sigmoid(x)\n",
    "    return x * sigmod_x\n",
    "\n",
    "def precompute_freqs_cis(head_dim: int, max_seq_len: int, rope_theta: int = 10000):\n",
    "    # [HD//2]\n",
    "    freqs = 1.0 / (rope_theta ** (np.arange(0, head_dim, 2)[: (head_dim // 2)] / head_dim))\n",
    "    # [M, HD//2]\n",
    "    freqs_for_each_token = np.outer(np.arange(max_seq_len), freqs)\n",
    "    freqs_cis = np.ones_like(freqs_for_each_token) * np.exp(1j * freqs_for_each_token)\n",
    "    return freqs_cis\n",
    "\n",
    "# 将向量转为复数表示\n",
    "def view_as_complex(real_np):\n",
    "    shape = real_np.shape\n",
    "    if shape[-1]!=2:\n",
    "        raise ValueError(\"Last dimension size must be 2 to represent real and imaginary parts.\")\n",
    "    complex_np =real_np[...,0] + 1j * real_np[..., 1]\n",
    "    return complex_np\n",
    "\n",
    "# 将向量转为实数表示\n",
    "def view_as_real(complex_np):\n",
    "    # 获取复数张量的形状\n",
    "    shape = complex_np.shape\n",
    "    # 创建一个形状为 (...,2) 的新数组，用于存储实部和虚部\n",
    "    # real_np = np.zeros(shape + (2,), dtype=complex_np.dtype)\n",
    "    real_np = np.zeros(shape + (2,), dtype=float)\n",
    "    # 将复数数组的实部和虚部分别存储到新数组的最后一个维度\n",
    "    real_np[..., 0] = np.real(complex_np)\n",
    "    real_np[..., 1] = np.imag(complex_np)\n",
    "    return real_np\n",
    "\n",
    "def view_as_real_gpu(complex_np):\n",
    "    # 获取复数张量的形状\n",
    "    shape = complex_np.shape\n",
    "    # 创建一个形状为 (...,2) 的新数组，用于存储实部和虚部\n",
    "    # real_np = np.zeros(shape + (2,), dtype=complex_np.dtype)\n",
    "    real_np = cp.zeros(shape + (2,), dtype=float)\n",
    "    # 将复数数组的实部和虚部分别存储到新数组的最后一个维度\n",
    "    real_np[..., 0] = cp.real(complex_np)\n",
    "    real_np[..., 1] = cp.imag(complex_np)\n",
    "    return real_np\n",
    "\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    # [\"B, L or 1, QHN,  HD\"] -> [\"B, L or 1, QHN,   HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    # [\"B, L or 1, KVHN, HD\"] -> [\"B, L or 1, KVHN,  HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    xq_ = view_as_complex(xq.reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = view_as_complex(xk.reshape(*xk.shape[:-1], -1, 2))\n",
    "    # [\"M, HD//2\"] -> [\"1, 1, M, HD//2\"]\n",
    "    freqs_cis = np.expand_dims(freqs_cis, axis=(0,2))\n",
    "    # 2.将query和key与频率进行点积\n",
    "    # [\"B, L or 1, QHN,  HD//2\"] * [\"1, 1, M, HD//2\"] -> [\"B, L or 1, QHN,  M\"]\n",
    "    xq_out_split = xq_ * freqs_cis\n",
    "    xk_out_split = xk_ * freqs_cis\n",
    "    # 将旋转后的query和key转换回实数表示\n",
    "    xq_out_split = view_as_real(xq_out_split)\n",
    "    xk_out_split = view_as_real(xk_out_split)\n",
    "    # 将旋转后的query和key合并回原来的形状\n",
    "    xq_out = xq_out_split.reshape(*xq.shape[:-1], -1)\n",
    "    xk_out = xk_out_split.reshape(*xk.shape[:-1], -1)\n",
    "    return xq_out, xk_out\n",
    "\n",
    "def apply_rotary_emb_gpu(xq, xk, freqs_cis):\n",
    "    xq_ = view_as_complex(xq.reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = view_as_complex(xk.reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = cp.expand_dims(freqs_cis, axis=(0,2))\n",
    "    xq_out_split = xq_ * freqs_cis\n",
    "    xk_out_split = xk_ * freqs_cis\n",
    "    xq_out_split = view_as_real_gpu(xq_out_split)\n",
    "    xk_out_split = view_as_real_gpu(xk_out_split)\n",
    "    xq_out = xq_out_split.reshape(*xq.shape[:-1], -1)\n",
    "    xk_out = xk_out_split.reshape(*xk.shape[:-1], -1)\n",
    "    return xq_out, xk_out\n",
    "\n",
    "\n",
    "def repeat_kv(x, n_rep: int):\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    xs = np.repeat(x, n_rep, axis=2)\n",
    "    return xs\n",
    "def repeat_kv_gpu(x, n_rep: int):\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    xs = cp.repeat(x, n_rep, axis=2)\n",
    "    return xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm:\n",
    "    def __init__(self, weight, eps: float):\n",
    "        self.eps = eps\n",
    "        self.weight = weight\n",
    "    def forward(self, x):\n",
    "        # 计算向量的平方平均值  [B, L or 1, D] -> [B, L or 1, 1]\n",
    "        squared_mean = np.mean(x**2, axis=-1, keepdims=True)\n",
    "        # 计算向量的均方根      [B, L or 1, 1]\n",
    "        rms = np.sqrt(squared_mean + self.eps)\n",
    "        # 计算归一化权重        [B, L or 1, D]\n",
    "        normalized_weight = x * self.weight / rms\n",
    "        return normalized_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, up_weight, gate_weight, down_weight):\n",
    "        self.up_weight = up_weight.T     # w3\n",
    "        self.gate_weight = gate_weight.T # w1\n",
    "        self.down_weight = down_weight.T # w2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FD = 14336\n",
    "        # [B, L or 1, D] @ [D, 14336] -> [B, L or 1, 14336]\n",
    "        swish = silu(x @ self.gate_weight)\n",
    "        # [B, L or 1, D] @ [D, FD] -> [B, L or 1, FD]\n",
    "        x_V = x @ self.up_weight\n",
    "        # [B, L or 1, FD] @ [B, L or 1, FD] -> [B, L or 1, FD]\n",
    "        x = swish * x_V\n",
    "        # [B, L or 1, FD] @ [FD, D] -> [B, L or 1, D]\n",
    "        x = x @ self.down_weight\n",
    "        return x\n",
    "\n",
    "class FeedForward_gpu:\n",
    "    def __init__(self, up_weight, gate_weight, down_weight):\n",
    "        self.up_weight = cp.asarray(up_weight.T)     # w3\n",
    "        self.gate_weight = cp.asarray(gate_weight.T) # w1\n",
    "        self.down_weight = cp.asarray(down_weight.T) # w2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = cp.asarray(x)\n",
    "        # FD = 14336\n",
    "        # [B, L or 1, D] @ [D, 14336] -> [B, L or 1, 14336]\n",
    "        swish = silu_gpu(x @ self.gate_weight)\n",
    "        # [B, L or 1, D] @ [D, FD] -> [B, L or 1, FD]\n",
    "        x_V = x @ self.up_weight\n",
    "        # [B, L or 1, FD] @ [B, L or 1, FD] -> [B, L or 1, FD]\n",
    "        x = swish * x_V\n",
    "        # [B, L or 1, FD] @ [FD, D] -> [B, L or 1, D]\n",
    "        x = x @ self.down_weight\n",
    "        return cp.asnumpy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_gpu:\n",
    "    def __init__(self, wq, wk, wv, wo, args):\n",
    "        # KVHN\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        # QHN, HN\n",
    "        self.n_heads = args.n_heads\n",
    "        # 每个KV头共享的Q头的个数 SHD = 4\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        # D // HN = 4096 // 32 = 128\n",
    "        self.head_dim = args.dim // self.n_heads\n",
    "        # wq: [D, D], wk: [D // 4, D], wv: [D  // 4, D], wo: [D, D]\n",
    "        self.wq = cp.asarray(wq.T)\n",
    "        self.wk = cp.asarray(wk.T)\n",
    "        self.wv = cp.asarray(wv.T)\n",
    "        self.wo = cp.asarray(wo.T)\n",
    "        \n",
    "        self.cache_k = cp.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = cp.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # x: [B, L or 1, D]\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        x = cp.asarray(x)\n",
    "        # xq: [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        # xk: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        # xv: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        xq = x @ self.wq\n",
    "        xk = x @ self.wk\n",
    "        xv = x @ self.wv\n",
    "        # 维度转换，将注意力头分离\n",
    "        # xq: [B, L or 1, D]      -> [B, L or 1, HN, HD]    [1, 1, 32, 128]\n",
    "        # xk: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        # xv: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        xq = xq.reshape(B, L, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb_gpu(xq, xk, freqs_cis)\n",
    "\n",
    "        # KV Cache\n",
    "        self.cache_k[:B, start_pos:start_pos + L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos + L] = xv\n",
    "        # ks: [B, L, KVHN, HD], vs: [B, L, KVHN, HD]\n",
    "        ks = self.cache_k[:B, start_pos:start_pos + L]\n",
    "        vs = self.cache_v[:B, start_pos:start_pos + L]\n",
    "        \n",
    "        # GQA\n",
    "        # xk: [B, L, HN, HD], xv: [B, L, HN, HD]\n",
    "        xk = repeat_kv_gpu(ks, self.n_rep)\n",
    "        xv = repeat_kv_gpu(vs, self.n_rep)\n",
    "\n",
    "        # [B, L, HN, HD] -> [B, HN, L, HD]\n",
    "        xq = xq.transpose(0, 2, 1, 3)\n",
    "        xk = xk.transpose(0, 2, 1, 3)\n",
    "        xv = xv.transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled Dot-Product Attention 乘和缩放\n",
    "        # [B, HN, L or 1, HD] @ [B, HN, HD, L] -> [B, HN, L or 1, L]\n",
    "        attention = xq @ xk.transpose(0, 1, 3, 2) / cp.sqrt(self.head_dim)\n",
    "        # `mask` is used only once at the beginning.\n",
    "        if mask is not None:\n",
    "            attention = attention + mask[None, None, :, :]\n",
    "        attention = softmax(attention)\n",
    "        # [B, HN, L or 1, L] @ [B, HN, L, HD] -> [B, HN, L or 1, HD]\n",
    "        output = attention @ xv\n",
    "\n",
    "        # [B, HN, L or 1, HD] -> [B, L or 1, HN, HD] -> [B, L or 1, D]\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n",
    "        # [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        output = output @ self.wo\n",
    "        return cp.asnumpy(output)\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self, wq, wk, wv, wo, args):\n",
    "        # KVHN\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        # QHN, HN\n",
    "        self.n_heads = args.n_heads\n",
    "        # 每个KV头共享的Q头的个数 SHD = 4\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        # D // HN = 4096 // 32 = 128\n",
    "        self.head_dim = args.dim // self.n_heads\n",
    "        # wq: [D, D], wk: [D // 4, D], wv: [D  // 4, D], wo: [D, D]\n",
    "        self.wq = wq.T\n",
    "        self.wk = wk.T\n",
    "        self.wv = wv.T\n",
    "        self.wo = wo.T\n",
    "        \n",
    "        self.cache_k = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # x: [B, L or 1, D]\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        # xq: [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        # xk: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        # xv: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        xq = x @ self.wq\n",
    "        xk = x @ self.wk\n",
    "        xv = x @ self.wv\n",
    "        # 维度转换，将注意力头分离\n",
    "        # xq: [B, L or 1, D]      -> [B, L or 1, HN, HD]    [1, 1, 32, 128]\n",
    "        # xk: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        # xv: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        xq = xq.reshape(B, L, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "\n",
    "        # KV Cache\n",
    "        self.cache_k[:B, start_pos:start_pos + L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos + L] = xv\n",
    "        # ks: [B, L, KVHN, HD], vs: [B, L, KVHN, HD]\n",
    "        ks = self.cache_k[:B, start_pos:start_pos + L]\n",
    "        vs = self.cache_v[:B, start_pos:start_pos + L]\n",
    "        \n",
    "        # GQA\n",
    "        # xk: [B, L, HN, HD], xv: [B, L, HN, HD]\n",
    "        xk = repeat_kv(ks, self.n_rep)\n",
    "        xv = repeat_kv(vs, self.n_rep)\n",
    "\n",
    "        # [B, L, HN, HD] -> [B, HN, L, HD]\n",
    "        xq = xq.transpose(0, 2, 1, 3)\n",
    "        xk = xk.transpose(0, 2, 1, 3)\n",
    "        xv = xv.transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled Dot-Product Attention 乘和缩放\n",
    "        # [B, HN, L or 1, HD] @ [B, HN, HD, L] -> [B, HN, L or 1, L]\n",
    "        attention = xq @ xk.transpose(0, 1, 3, 2) / np.sqrt(self.head_dim)\n",
    "        # `mask` is used only once at the beginning.\n",
    "        if mask is not None:\n",
    "            attention = attention + mask[None, None, :, :]\n",
    "        attention = softmax(attention)\n",
    "        # [B, HN, L or 1, L] @ [B, HN, L, HD] -> [B, HN, L or 1, HD]\n",
    "        output = attention @ xv\n",
    "\n",
    "        # [B, HN, L or 1, HD] -> [B, L or 1, HN, HD] -> [B, L or 1, D]\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n",
    "        # [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        output = output @ self.wo\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock:\n",
    "    def __init__(self, layer_weights: dict, layer_id: int, args: ModelArgs):\n",
    "        \n",
    "        self.before_attention_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.attention_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wq.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wk.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wv.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wo.weight\"],\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        self.before_ffn_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.ffn_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w3.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w1.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w2.weight\"]            \n",
    "        )\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # Attention---------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        norm_x = self.before_attention_rms_norm.forward(x)\n",
    "        # Masked Multi-Head Attention\n",
    "        # [B, L or 1, D]\n",
    "        h1 = self.attention.forward(norm_x, start_pos, mask, freqs_cis)\n",
    "        z = x + h1\n",
    "\n",
    "        # Feed Forward----------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        norm_z = self.before_ffn_rms_norm.forward(z)\n",
    "        # Feed Forward + SwiGLU\n",
    "        # [B, L or 1, D]\n",
    "        h2 = self.feed_forward.forward(norm_z)\n",
    "        out = z + h2\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama:\n",
    "    def __init__(self, Model_Home, args):\n",
    "        self.args = args\n",
    "        \n",
    "        self.home = Model_Home\n",
    "        # tok_embeding: [VS, D]\n",
    "        self.tok_embeding = np.load(self.home + \"shuke/llama3.8b.shuke.tok_embeddings.weight.npz\")['tok_embeddings.weight']\n",
    "\n",
    "        # RoPE\n",
    "        self.freqs_cis = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "\n",
    "        self.layers = []\n",
    "        for layer_id in range(args.n_layers):\n",
    "            layer_weights = np.load(self.home + f\"shuke/llama3.8b.shuke.layer.{layer_id}.npz\")\n",
    "            self.layers.append(TransformerBlock(layer_weights, layer_id, args))\n",
    "\n",
    "        self.norm = RMSNorm(\n",
    "            np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        # [D,VS]\n",
    "        self.wo = np.load(self.home + \"shuke/llama3.8b.shuke.output.weight.npz\")['output.weight'].T\n",
    "\n",
    "    def forward(self, input_ids, start_pos: int):\n",
    "        _, L = input_ids.shape\n",
    "        # [B, L or 1, D]\n",
    "        h = self.tok_embeding[input_ids]\n",
    "        # 构建掩码mask: [L, L]，只在开始第一次时才构建mask\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = np.full((L, L), float('-inf'))\n",
    "            mask = np.triu(mask, k=1)\n",
    "            mask = np.concatenate([np.zeros((L, start_pos)), mask], axis=1)\n",
    "        \n",
    "        # Transformer Layers\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # [B, L or 1, D]\n",
    "            h = layer.forward(h, start_pos, mask, freqs_cis)\n",
    "        \n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        h = self.norm.forward(h)\n",
    "        # Only forward the output from the last position.\n",
    "        # [B, 1, VS] = [B, 1(L), D] @ [D, VS]\n",
    "        logit = h[:,[-1],:] @ self.wo\n",
    "        return logit\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens: int):\n",
    "        L = len(input_ids)\n",
    "        for i, curr_pos in enumerate(range(L, max_new_tokens)):\n",
    "            if i == 0: # Prefill Phase 第一次解码\n",
    "                inputs = input_ids\n",
    "                pos = 0\n",
    "            else: # Decode Phase 逐个解码\n",
    "                inputs = next_id\n",
    "                pos = curr_pos\n",
    "            # [B, 1, VS]\n",
    "            logits = self.forward(inputs, pos)\n",
    "            next_id = logits[:,-1,:].argmax(-1,keepdims=True)\n",
    "            yield next_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东大学（威海）， 201\n",
      "用时: 246.91s"
     ]
    }
   ],
   "source": [
    "# 初始化参数\n",
    "\n",
    "class Llama:\n",
    "    def __init__(self, Model_Home, args):\n",
    "        self.args = args\n",
    "        \n",
    "        self.home = Model_Home\n",
    "        # tok_embeding: [VS, D]\n",
    "        self.tok_embeding = np.load(self.home + \"shuke/llama3.8b.shuke.tok_embeddings.weight.npz\")['tok_embeddings.weight']\n",
    "\n",
    "        # RoPE\n",
    "        self.freqs_cis = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "\n",
    "        self.layers = []\n",
    "        for layer_id in range(args.n_layers):\n",
    "            layer_weights = np.load(self.home + f\"shuke/llama3.8b.shuke.layer.{layer_id}.npz\")\n",
    "            self.layers.append(TransformerBlock(layer_weights, layer_id, args))\n",
    "\n",
    "        self.norm = RMSNorm(\n",
    "            np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        # [D,VS]\n",
    "        self.wo = np.load(self.home + \"shuke/llama3.8b.shuke.output.weight.npz\")['output.weight'].T\n",
    "\n",
    "    def forward(self, input_ids, start_pos: int):\n",
    "        _, L = input_ids.shape\n",
    "        # [B, L or 1, D]\n",
    "        h = self.tok_embeding[input_ids]\n",
    "        # 构建掩码mask: [L, L]，只在开始第一次时才构建mask\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = np.full((L, L), float('-inf'))\n",
    "            mask = np.triu(mask, k=1)\n",
    "            mask = np.concatenate([np.zeros((L, start_pos)), mask], axis=1)\n",
    "        \n",
    "        # Transformer Layers\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # [B, L or 1, D]\n",
    "            h = layer.forward(h, start_pos, mask, freqs_cis)\n",
    "        \n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        h = self.norm.forward(h)\n",
    "        # Only forward the output from the last position.\n",
    "        # [B, 1, VS] = [B, 1(L), D] @ [D, VS]\n",
    "        logit = h[:,[-1],:] @ self.wo\n",
    "        return logit\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens: int):\n",
    "        L = len(input_ids)\n",
    "        for i, curr_pos in enumerate(range(L, max_new_tokens)):\n",
    "            if i == 0: # Prefill Phase 第一次解码\n",
    "                inputs = input_ids\n",
    "                pos = 0\n",
    "            else: # Decode Phase 逐个解码\n",
    "                inputs = next_id\n",
    "                pos = curr_pos\n",
    "            # [B, 1, VS]\n",
    "            logits = self.forward(inputs, pos)\n",
    "            next_id = logits[:,-1,:].argmax(-1,keepdims=True)\n",
    "            yield next_id\n",
    "if __name__ == \"__main__\":\n",
    "    args = ModelArgs()\n",
    "    Model_Home = \"../Meta-Llama-3/Meta-Llama-3-8B/original/\"\n",
    "\n",
    "    tokenizer = Tokenizer(Model_Home + \"tokenizer.model\")\n",
    "    model = Llama(Model_Home, args)\n",
    "    \n",
    "    # prompt = \"What is the capital of France?\"\n",
    "    # prompt = \"the answer to the ultimate question of life, the universe, and everything is \" # 42\n",
    "    # prompt = \"the answer to the ultimate question of life, the universe, and everything is \" # 42\n",
    "    prompt = \"山东大学（威海）， \"\n",
    "    print(f\"{prompt}\", end=\"\")\n",
    "    input_ids = [128000] + tokenizer.encode(prompt)\n",
    "    input_ids = np.expand_dims(input_ids,0)\n",
    "    start = time.time()\n",
    "    _, L = input_ids.shape\n",
    "    for id in model.generate(input_ids, args.max_new_tokens):\n",
    "        L += 1\n",
    "        output_id = id[0].tolist()\n",
    "        if output_id[-1] in [1, 2]:\n",
    "            break\n",
    "        print(tokenizer.decode(output_id), end=\"\")\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"\\n用时: {elapsed:.2f}s\", end=\"\")\n",
    "        start = time.time()\n",
    "        sys.stdout.flush()\n",
    "        break\n",
    "    # elapsed = time.time() - start\n",
    "    # print(f\"\\n\\nToken count: {L}, elapsed: {elapsed:.2f}s, {round(L/elapsed, )} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cpu + Attention GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock_gpu:\n",
    "    def __init__(self, layer_weights: dict, layer_id: int, args: ModelArgs):\n",
    "        \n",
    "        self.before_attention_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.attention_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.attention = Attention_gpu(\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wq.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wk.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wv.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wo.weight\"],\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        self.before_ffn_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.ffn_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w3.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w1.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w2.weight\"]            \n",
    "        )\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # Attention---------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        norm_x = self.before_attention_rms_norm.forward(x)\n",
    "        # Masked Multi-Head Attention\n",
    "        # [B, L or 1, D]\n",
    "        h1 = self.attention.forward(norm_x, start_pos, mask, freqs_cis)\n",
    "        z = x + h1\n",
    "\n",
    "        # Feed Forward----------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        norm_z = self.before_ffn_rms_norm.forward(z)\n",
    "        # Feed Forward + SwiGLU\n",
    "        # [B, L or 1, D]\n",
    "        h2 = self.feed_forward.forward(norm_z)\n",
    "        out = z + h2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东大学（威海）， 201\n",
      "用时: 207.19s"
     ]
    }
   ],
   "source": [
    "class Llama:\n",
    "    def __init__(self, Model_Home, args):\n",
    "        self.args = args\n",
    "        \n",
    "        self.home = Model_Home\n",
    "        # tok_embeding: [VS, D]\n",
    "        self.tok_embeding = np.load(self.home + \"shuke/llama3.8b.shuke.tok_embeddings.weight.npz\")['tok_embeddings.weight']\n",
    "\n",
    "        # RoPE\n",
    "        self.freqs_cis = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "\n",
    "        self.layers = []\n",
    "        for layer_id in range(args.n_layers):\n",
    "            layer_weights = np.load(self.home + f\"shuke/llama3.8b.shuke.layer.{layer_id}.npz\")\n",
    "            self.layers.append(TransformerBlock(layer_weights, layer_id, args))\n",
    "\n",
    "        self.norm = RMSNorm(\n",
    "            np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        # [D,VS]\n",
    "        self.wo = np.load(self.home + \"shuke/llama3.8b.shuke.output.weight.npz\")['output.weight'].T\n",
    "\n",
    "    def forward(self, input_ids, start_pos: int):\n",
    "        _, L = input_ids.shape\n",
    "        # [B, L or 1, D]\n",
    "        h = self.tok_embeding[input_ids]\n",
    "        # 构建掩码mask: [L, L]，只在开始第一次时才构建mask\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = np.full((L, L), float('-inf'))\n",
    "            mask = np.triu(mask, k=1)\n",
    "            mask = np.concatenate([np.zeros((L, start_pos)), mask], axis=1)\n",
    "        \n",
    "        # Transformer Layers\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # [B, L or 1, D]\n",
    "            h = layer.forward(h, start_pos, mask, freqs_cis)\n",
    "        \n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        h = self.norm.forward(h)\n",
    "        # Only forward the output from the last position.\n",
    "        # [B, 1, VS] = [B, 1(L), D] @ [D, VS]\n",
    "        logit = h[:,[-1],:] @ self.wo\n",
    "        return logit\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens: int):\n",
    "        L = len(input_ids)\n",
    "        for i, curr_pos in enumerate(range(L, max_new_tokens)):\n",
    "            if i == 0: # Prefill Phase 第一次解码\n",
    "                inputs = input_ids\n",
    "                pos = 0\n",
    "            else: # Decode Phase 逐个解码\n",
    "                inputs = next_id\n",
    "                pos = curr_pos\n",
    "            # [B, 1, VS]\n",
    "            logits = self.forward(inputs, pos)\n",
    "            next_id = logits[:,-1,:].argmax(-1,keepdims=True)\n",
    "            yield next_id\n",
    "if __name__ == \"__main__\":\n",
    "    args = ModelArgs()\n",
    "    Model_Home = \"../Meta-Llama-3/Meta-Llama-3-8B/original/\"\n",
    "\n",
    "    tokenizer = Tokenizer(Model_Home + \"tokenizer.model\")\n",
    "    model = Llama(Model_Home, args)\n",
    "    \n",
    "    # prompt = \"the answer to the ultimate question of life, the universe, and everything is \" # 42\n",
    "    prompt = \"山东大学（威海）， \"\n",
    "    print(f\"{prompt}\", end=\"\")\n",
    "    input_ids = [128000] + tokenizer.encode(prompt)\n",
    "    input_ids = np.expand_dims(input_ids,0)\n",
    "    start = time.time()\n",
    "    _, L = input_ids.shape\n",
    "    for id in model.generate(input_ids, args.max_new_tokens):\n",
    "        L += 1\n",
    "        output_id = id[0].tolist()\n",
    "        if output_id[-1] in [1, 2]:\n",
    "            break\n",
    "        print(tokenizer.decode(output_id), end=\"\")\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"\\n用时: {elapsed:.2f}s\", end=\"\")\n",
    "        start = time.time()\n",
    "        sys.stdout.flush()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cpu + AttentionGPU + FFNGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock_gpu:\n",
    "    def __init__(self, layer_weights: dict, layer_id: int, args: ModelArgs):\n",
    "        \n",
    "        self.before_attention_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.attention_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.attention = Attention_gpu(\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wq.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wk.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wv.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wo.weight\"],\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        self.before_ffn_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.ffn_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward_gpu(\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w3.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w1.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w2.weight\"]            \n",
    "        )\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # Attention---------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        norm_x = self.before_attention_rms_norm.forward(x)\n",
    "        # Masked Multi-Head Attention\n",
    "        # [B, L or 1, D]\n",
    "        h1 = self.attention.forward(norm_x, start_pos, mask, freqs_cis)\n",
    "        z = x + h1\n",
    "\n",
    "        # Feed Forward----------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        norm_z = self.before_ffn_rms_norm.forward(z)\n",
    "        # Feed Forward + SwiGLU\n",
    "        # [B, L or 1, D]\n",
    "        h2 = self.feed_forward.forward(norm_z)\n",
    "        out = z + h2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东大学（威海）， 201\n",
      "用时: 251.98s"
     ]
    }
   ],
   "source": [
    "class Llama:\n",
    "    def __init__(self, Model_Home, args):\n",
    "        self.args = args\n",
    "        \n",
    "        self.home = Model_Home\n",
    "        # tok_embeding: [VS, D]\n",
    "        self.tok_embeding = np.load(self.home + \"shuke/llama3.8b.shuke.tok_embeddings.weight.npz\")['tok_embeddings.weight']\n",
    "\n",
    "        # RoPE\n",
    "        self.freqs_cis = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "\n",
    "        self.layers = []\n",
    "        for layer_id in range(args.n_layers):\n",
    "            layer_weights = np.load(self.home + f\"shuke/llama3.8b.shuke.layer.{layer_id}.npz\")\n",
    "            self.layers.append(TransformerBlock(layer_weights, layer_id, args))\n",
    "\n",
    "        self.norm = RMSNorm(\n",
    "            np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        # [D,VS]\n",
    "        self.wo = np.load(self.home + \"shuke/llama3.8b.shuke.output.weight.npz\")['output.weight'].T\n",
    "\n",
    "    def forward(self, input_ids, start_pos: int):\n",
    "        _, L = input_ids.shape\n",
    "        # [B, L or 1, D]\n",
    "        h = self.tok_embeding[input_ids]\n",
    "        # 构建掩码mask: [L, L]，只在开始第一次时才构建mask\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = np.full((L, L), float('-inf'))\n",
    "            mask = np.triu(mask, k=1)\n",
    "            mask = np.concatenate([np.zeros((L, start_pos)), mask], axis=1)\n",
    "        \n",
    "        # Transformer Layers\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+L]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # [B, L or 1, D]\n",
    "            h = layer.forward(h, start_pos, mask, freqs_cis)\n",
    "        \n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        h = self.norm.forward(h)\n",
    "        # Only forward the output from the last position.\n",
    "        # [B, 1, VS] = [B, 1(L), D] @ [D, VS]\n",
    "        logit = h[:,[-1],:] @ self.wo\n",
    "        return logit\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens: int):\n",
    "        L = len(input_ids)\n",
    "        for i, curr_pos in enumerate(range(L, max_new_tokens)):\n",
    "            if i == 0: # Prefill Phase 第一次解码\n",
    "                inputs = input_ids\n",
    "                pos = 0\n",
    "            else: # Decode Phase 逐个解码\n",
    "                inputs = next_id\n",
    "                pos = curr_pos\n",
    "            # [B, 1, VS]\n",
    "            logits = self.forward(inputs, pos)\n",
    "            next_id = logits[:,-1,:].argmax(-1,keepdims=True)\n",
    "            yield next_id\n",
    "if __name__ == \"__main__\":\n",
    "    args = ModelArgs()\n",
    "    Model_Home = \"../Meta-Llama-3/Meta-Llama-3-8B/original/\"\n",
    "\n",
    "    tokenizer = Tokenizer(Model_Home + \"tokenizer.model\")\n",
    "    model = Llama(Model_Home, args)\n",
    "    \n",
    "    # prompt = \"the answer to the ultimate question of life, the universe, and everything is \" # 42\n",
    "    prompt = \"山东大学（威海）， \"\n",
    "    print(f\"{prompt}\", end=\"\")\n",
    "    input_ids = [128000] + tokenizer.encode(prompt)\n",
    "    input_ids = np.expand_dims(input_ids,0)\n",
    "    start = time.time()\n",
    "    _, L = input_ids.shape\n",
    "    for id in model.generate(input_ids, args.max_new_tokens):\n",
    "        L += 1\n",
    "        output_id = id[0].tolist()\n",
    "        if output_id[-1] in [1, 2]:\n",
    "            break\n",
    "        print(tokenizer.decode(output_id), end=\"\")\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"\\n用时: {elapsed:.2f}s\", end=\"\")\n",
    "        start = time.time()\n",
    "        sys.stdout.flush()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
