{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama3 Lora 微调\n",
    "在这个文件中，我使用 LORA 对 Llama3-8B 模型进行了指令微调。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "Model_Home = \"../Meta-Llama-3/Meta-Llama-3-8B/original/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = Model_Home + \"tokenizer.model\"\n",
    "special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
    "\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 4096, 'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8, 'vocab_size': 128256, 'multiple_of': 1024, 'ffn_dim_multiplier': 1.3, 'norm_eps': 1e-05, 'rope_theta': 500000.0}\n"
     ]
    }
   ],
   "source": [
    "# 查看模型参数文件\n",
    "with open(Model_Home + \"params.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "dim = config[\"dim\"]\n",
    "n_layers = config[\"n_layers\"]\n",
    "n_heads = config[\"n_heads\"]\n",
    "n_kv_heads = config[\"n_kv_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "multiple_of = config[\"multiple_of\"]\n",
    "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
    "norm_eps = config[\"norm_eps\"]\n",
    "rope_theta = config[\"rope_theta\"]\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文本转换为 **词元(tokens)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 58911, 68464, 102667, 10110, 105578, 56235, 7705, 48785, 70626, 128001]\n",
      "['<|begin_of_text|>', '山', '东', '大学', '（', '威', '海', '）', ' 数', '科', '<|end_of_text|>']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "prompt = \"山东大学（威海） 数科\" #\n",
    "\n",
    "tokens = [128000] + tokenizer.encode(prompt) + [128001]\n",
    "print(tokens)\n",
    "prompt_split_as_tokens = [tokenizer.decode([token]) for token in tokens]\n",
    "print(prompt_split_as_tokens)\n",
    "print(len(prompt_split_as_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换为 **嵌入(embedding)**\n",
    "这是这个代码文件中中唯一使用内置神经网络模块的地方的部分\n",
    "<br>\n",
    "我们的 [Nx1] 词元现在将变成 [Nx4096]，即 N 个嵌入（每个词元）长度为 4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_embeddings_weight = np.load(Model_Home + \"shuke/llama3.8b.shuke.tok_embeddings.weight.npz\")\n",
    "# token_embeddings_unnormalized用tokens列表里的行数作为tok_embeddings_weight的行索引\n",
    "token_embeddings_unnormalized = tok_embeddings_weight['tok_embeddings.weight'][tokens]\n",
    "del tok_embeddings_weight\n",
    "token_embeddings_unnormalized.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RMS 归一化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(tensor_np, norm_weights_np):\n",
    "    # 计算张量的平方平均值\n",
    "    squared_mean = np.mean(tensor_np ** 2, axis=-1, keepdims=True)\n",
    "    # 计算张量的均方根值\n",
    "    rms = np.sqrt(squared_mean + norm_eps)\n",
    "    # 计算归一化权重\n",
    "    normalized_weights = tensor_np * norm_weights_np / rms\n",
    "    return normalized_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positioning encoding 位置编码\n",
    "### RoPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       0.015625 0.03125  0.046875 0.0625   0.078125 0.09375  0.109375\n",
      " 0.125    0.140625 0.15625  0.171875 0.1875   0.203125 0.21875  0.234375\n",
      " 0.25     0.265625 0.28125  0.296875 0.3125   0.328125 0.34375  0.359375\n",
      " 0.375    0.390625 0.40625  0.421875 0.4375   0.453125 0.46875  0.484375\n",
      " 0.5      0.515625 0.53125  0.546875 0.5625   0.578125 0.59375  0.609375\n",
      " 0.625    0.640625 0.65625  0.671875 0.6875   0.703125 0.71875  0.734375\n",
      " 0.75     0.765625 0.78125  0.796875 0.8125   0.828125 0.84375  0.859375\n",
      " 0.875    0.890625 0.90625  0.921875 0.9375   0.953125 0.96875  0.984375]\n"
     ]
    }
   ],
   "source": [
    "zero_to_one_split_into_64_parts = np.array(range(64))/64\n",
    "print(zero_to_one_split_into_64_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 8.14617234e-01 6.63601238e-01 5.40581005e-01\n",
      " 4.40366603e-01 3.58730224e-01 2.92227823e-01 2.38053820e-01\n",
      " 1.93922745e-01 1.57972810e-01 1.28687373e-01 1.04830952e-01\n",
      " 8.53971003e-02 6.95659496e-02 5.66696214e-02 4.61640503e-02\n",
      " 3.76060309e-02 3.06345209e-02 2.49554087e-02 2.03291060e-02\n",
      " 1.65604401e-02 1.34904199e-02 1.09895285e-02 8.95225934e-03\n",
      " 7.29266474e-03 5.94073038e-03 4.83942135e-03 3.94227603e-03\n",
      " 3.21144599e-03 2.61609925e-03 2.13111954e-03 1.73604670e-03\n",
      " 1.41421356e-03 1.15204274e-03 9.38473870e-04 7.64496988e-04\n",
      " 6.22772422e-04 5.07321148e-04 4.13272550e-04 3.36658941e-04\n",
      " 2.74248176e-04 2.23407290e-04 1.81991429e-04 1.48253354e-04\n",
      " 1.20769737e-04 9.83811094e-05 8.01429472e-05 6.52858260e-05\n",
      " 5.31829590e-05 4.33237549e-05 3.52922774e-05 2.87496974e-05\n",
      " 2.34199990e-05 1.90783348e-05 1.55415403e-05 1.26604066e-05\n",
      " 1.03133854e-05 8.40146147e-06 6.84397530e-06 5.57522023e-06\n",
      " 4.54167048e-06 3.69972304e-06 3.01385815e-06 2.45514079e-06]\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "print(freqs)\n",
    "print(freqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算外积\n",
    "freqs_for_each_token = np.outer(np.arange(11), freqs)\n",
    "# cis(x)=cos(x)+i·sin(x)\n",
    "freqs_cis = np.ones_like(freqs_for_each_token) * np.exp(1j * freqs_for_each_token)\n",
    "freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将向量转为复数表示\n",
    "def view_as_complex(real_np):\n",
    "    shape = real_np.shape\n",
    "    # 确保最后一个维度是2，表示实部和虚部\n",
    "    if shape[-1]!=2:\n",
    "        raise ValueError(\"Last dimension size must be 2 to represent real and imaginary parts.\")\n",
    "    # 将最后一个维度合并为复数表示\n",
    "    complex_np =real_np[...,0] + 1j * real_np[..., 1]\n",
    "    return complex_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将向量转为实数表示\n",
    "def view_as_real(complex_np):\n",
    "    # 获取复数张量的形状\n",
    "    shape = complex_np.shape\n",
    "    # 创建一个形状为 (...,2) 的新数组，用于存储实部和虚部\n",
    "    # real_np = np.zeros(shape + (2,), dtype=complex_np.dtype)\n",
    "    real_np = np.zeros(shape + (2,), dtype=float)\n",
    "    # 将复数数组的实部和虚部分别存储到新数组的最后一个维度\n",
    "    real_np[..., 0] = np.real(complex_np)\n",
    "    real_np[..., 1] = np.imag(complex_np)\n",
    "    return real_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "旋转的查询对现在被合并，我们可以获得一个新的查询向量（旋转的查询向量），其形状为 [17x128]，其中 17 是词元(token)数，128 是查询向量的维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义softmax函数\n",
    "def softmax(X):\n",
    "    exp_x = np.exp(X - np.max(X, axis=-1, keepdims=True))\n",
    "    return exp_x/exp_x.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Numpy实现的silu函数（即Swish激活函数）\n",
    "def silu(x):\n",
    "    # 计算 sigmod(x)\n",
    "    sigmod_x = 1 / (1 + np.exp(-x))\n",
    "    # 计算 SiLU(x) = x* sigmoid(x)\n",
    "    return x * sigmod_x\n",
    "# silu(np.matmul(embedding_after_edit_normalized, w1.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对最后一个Transformer层的QK进行Lora微调\n",
    "\n",
    "## 先获得前 31个Transformer层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储嵌入\n",
    "final_embedding = token_embeddings_unnormalized\n",
    "# 对每层进行循环\n",
    "for layer in range(n_layers-1):\n",
    "    # 存储Q、K、V注意力得分\n",
    "    qkv_attention_store = []\n",
    "    # 获取每层的Q、K、V、O权重信息np.load(Model_Home + \"shuke/llama3.8b.shuke.layer.0.npz\")[\"layers.0.attention_norm.weight\"])\n",
    "    layer_weights = np.load(Model_Home + f\"shuke/llama3.8b.shuke.layer.{layer}.npz\")\n",
    "    # layer_embedding_norm = rms_norm(final_embedding, tensor_to_numpy(model[f\"layers.{layer}.attention_norm.weight\"]))\n",
    "    layer_embedding_norm = rms_norm(final_embedding, layer_weights[f\"layers.{layer}.attention_norm.weight\"])\n",
    "    # q_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wq.weight\"])\n",
    "    q_layer = layer_weights[f\"layers.{layer}.attention.wq.weight\"]\n",
    "    q_layer = q_layer.reshape(n_heads, q_layer.shape[0] // n_heads, dim)\n",
    "    # k_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wk.weight\"])\n",
    "    k_layer = layer_weights[f\"layers.{layer}.attention.wk.weight\"]\n",
    "    k_layer = k_layer.reshape(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
    "    # v_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wv.weight\"])\n",
    "    v_layer = layer_weights[f\"layers.{layer}.attention.wv.weight\"]\n",
    "    v_layer = v_layer.reshape(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
    "    # w_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wo.weight\"])\n",
    "    w_layer = layer_weights[f\"layers.{layer}.attention.wo.weight\"]\n",
    "    # 遍历所有注意力头\n",
    "    for head in range(n_heads):\n",
    "        # 获得第一层的每个head的Q、K、V的权重\n",
    "        q_layer_head = q_layer[head]\n",
    "        k_layer_head = k_layer[head//4]\n",
    "        v_layer_head = v_layer[head//4]\n",
    "        # 获得第一层的每个head的权重与embedings相乘\n",
    "        q_per_token = np.matmul(layer_embedding_norm, q_layer_head.T)\n",
    "        k_per_token = np.matmul(layer_embedding_norm, k_layer_head.T)\n",
    "        v_per_token = np.matmul(layer_embedding_norm, v_layer_head.T)\n",
    "        # 对Q、K进行旋转\n",
    "        q_per_token_split_into_pairs = q_per_token.reshape(q_per_token.shape[0], -1, 2)\n",
    "        q_per_token_as_complex_numbers = view_as_complex(q_per_token_split_into_pairs)\n",
    "        q_per_token_split_into_pairs_rotated = view_as_real(q_per_token_as_complex_numbers * freqs_cis)\n",
    "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.reshape(q_per_token.shape)\n",
    "        k_per_token_split_into_pairs = k_per_token.reshape(k_per_token.shape[0], -1, 2)\n",
    "        k_per_token_as_complex_numbers = view_as_complex(k_per_token_split_into_pairs)\n",
    "        k_per_token_split_into_pairs_rotated = view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.reshape(k_per_token.shape)\n",
    "        # 旋转后的Q、K相乘，获得自注意力得分\n",
    "        qk_per_token = np.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5\n",
    "        mask = np.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(\"-inf\"))\n",
    "        # 掩码操作\n",
    "        mask = np.triu(mask, k=1)\n",
    "        qk_per_token_after_masking = qk_per_token + mask\n",
    "        qk_per_token_after_masking_after_softmax = softmax(qk_per_token_after_masking)\n",
    "        qkv_attention = np.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "        # 与V相乘获得每个token的注意力得分\n",
    "        qkv_attention_store.append(qkv_attention)\n",
    "    # 合并，获得多头注意力得分\n",
    "    stacked_qkv_attention = np.concatenate(qkv_attention_store, axis=-1)\n",
    "    # w_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wo.weight\"])\n",
    "    embedding_delta = np.matmul(stacked_qkv_attention, w_layer.T)\n",
    "    embedding_after_edit = final_embedding + embedding_delta\n",
    "    # 归一化\n",
    "    # embedding_after_edit_normalized = rms_norm(embedding_after_edit, tensor_to_numpy(model[f\"layers.{layer}.ffn_norm.weight\"]))\n",
    "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, layer_weights[f\"layers.{layer}.ffn_norm.weight\"])\n",
    "    # SwiGLU 激活\n",
    "    w1 = layer_weights[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
    "    w2 = layer_weights[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
    "    w3 = layer_weights[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
    "    \n",
    "    output_after_feedforward = np.matmul(silu(np.matmul(embedding_after_edit_normalized, w1.T)) * np.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "    # 相加获得最终的embedding\n",
    "    final_embedding = embedding_after_edit+output_after_feedforward\n",
    "# 存储before_final_embedding\n",
    "# np.savez_compressed(\"before_final_embedding.npz\", final_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 4096)\n",
      "(11, 4096)\n"
     ]
    }
   ],
   "source": [
    "demo = True\n",
    "if demo:\n",
    "    before_final_embedding = final_embedding\n",
    "else:\n",
    "    before_final_embedding = np.load(\"before_final_embedding.npz\")[\"arr_0\"]\n",
    "    final_embedding = before_final_embedding[:-2]\n",
    "\n",
    "print(before_final_embedding.shape)\n",
    "print(final_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原模型输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单次TransformerBlock循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48864\n",
      "山东大学（威海） 数学\n"
     ]
    }
   ],
   "source": [
    "final_embedding = before_final_embedding[:-2]\n",
    "# 对每层进行循环\n",
    "for layer in range(n_layers-1,n_layers):\n",
    "    # 存储Q、K、V注意力得分\n",
    "    qkv_attention_store = []\n",
    "    # 获取每层的Q、K、V、O权重信息np.load(Model_Home + \"shuke/llama3.8b.shuke.layer.0.npz\")[\"layers.0.attention_norm.weight\"])\n",
    "    layer_weights = np.load(Model_Home + f\"shuke/llama3.8b.shuke.layer.{layer}.npz\")\n",
    "    # layer_embedding_norm = rms_norm(final_embedding, tensor_to_numpy(model[f\"layers.{layer}.attention_norm.weight\"]))\n",
    "    layer_embedding_norm = rms_norm(final_embedding, layer_weights[f\"layers.{layer}.attention_norm.weight\"])\n",
    "    # q_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wq.weight\"])\n",
    "    q_layer = layer_weights[f\"layers.{layer}.attention.wq.weight\"]\n",
    "    q_layer = q_layer.reshape(n_heads, q_layer.shape[0] // n_heads, dim)\n",
    "    # k_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wk.weight\"])\n",
    "    k_layer = layer_weights[f\"layers.{layer}.attention.wk.weight\"]\n",
    "    k_layer = k_layer.reshape(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
    "    # v_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wv.weight\"])\n",
    "    v_layer = layer_weights[f\"layers.{layer}.attention.wv.weight\"]\n",
    "    v_layer = v_layer.reshape(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
    "    # w_layer = tensor_to_numpy(model[f\"layers.{layer}.attention.wo.weight\"])\n",
    "    w_layer = layer_weights[f\"layers.{layer}.attention.wo.weight\"]\n",
    "    # 遍历所有注意力头\n",
    "    for head in range(n_heads):\n",
    "        # 获得第一层的每个head的Q、K、V的权重\n",
    "        q_layer_head = q_layer[head]\n",
    "        k_layer_head = k_layer[head//4]\n",
    "        v_layer_head = v_layer[head//4]\n",
    "        # 获得第一层的每个head的权重与embedings相乘\n",
    "        q_per_token = np.matmul(layer_embedding_norm, q_layer_head.T)\n",
    "        k_per_token = np.matmul(layer_embedding_norm, k_layer_head.T)\n",
    "        v_per_token = np.matmul(layer_embedding_norm, v_layer_head.T)\n",
    "        # 对Q、K进行旋转\n",
    "        q_per_token_split_into_pairs = q_per_token.reshape(q_per_token.shape[0], -1, 2)\n",
    "        q_per_token_as_complex_numbers = view_as_complex(q_per_token_split_into_pairs)\n",
    "        q_per_token_split_into_pairs_rotated = view_as_real(q_per_token_as_complex_numbers * freqs_cis[:-2])\n",
    "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.reshape(q_per_token.shape)\n",
    "        k_per_token_split_into_pairs = k_per_token.reshape(k_per_token.shape[0], -1, 2)\n",
    "        k_per_token_as_complex_numbers = view_as_complex(k_per_token_split_into_pairs)\n",
    "        k_per_token_split_into_pairs_rotated = view_as_real(k_per_token_as_complex_numbers * freqs_cis[:-2])\n",
    "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.reshape(k_per_token.shape)\n",
    "        # 旋转后的Q、K相乘，获得自注意力得分\n",
    "        qk_per_token = np.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5\n",
    "        mask = np.full((len(before_final_embedding[:-2]), len(before_final_embedding[:-2])), float(\"-inf\"))\n",
    "        # 掩码操作\n",
    "        mask = np.triu(mask, k=1)\n",
    "        qk_per_token_after_masking = qk_per_token + mask\n",
    "        qk_per_token_after_masking_after_softmax = softmax(qk_per_token_after_masking)\n",
    "        qkv_attention = np.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "        # 与V相乘获得每个token的注意力得分\n",
    "        qkv_attention_store.append(qkv_attention)\n",
    "    # 合并，获得多头注意力\n",
    "    stacked_qkv_attention = np.concatenate(qkv_attention_store, axis=-1)\n",
    "    embedding_delta = np.matmul(stacked_qkv_attention, w_layer.T)\n",
    "    embedding_after_edit = final_embedding + embedding_delta\n",
    "    # 归一化\n",
    "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, layer_weights[f\"layers.{layer}.ffn_norm.weight\"])\n",
    "    # SwiGLU 激活\n",
    "    w1 = layer_weights[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
    "    w2 = layer_weights[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
    "    w3 = layer_weights[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
    "    \n",
    "    output_after_feedforward = np.matmul(silu(np.matmul(embedding_after_edit_normalized, w1.T)) * np.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "    # 相加获得最终的embedding\n",
    "    final_embedding = embedding_after_edit+output_after_feedforward\n",
    "# 归一化\n",
    "final_embedding = rms_norm(final_embedding, np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"])\n",
    "# 最终的线性层相乘\n",
    "logits = np.matmul(final_embedding[-1], np.load(Model_Home + f\"shuke/llama3.8b.shuke.output.weight.npz\")[\"output.weight\"].T)\n",
    "next_token = np.argmax(logits, axis=-1)\n",
    "print(next_token)\n",
    "print(f\"{prompt[:-1]}\", end=\"\")\n",
    "print(tokenizer.decode([next_token.item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装成类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ModelArgs\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    # 将query和key复数化，具体就是将dim维度分成两半，每一半是dim/2维，分别用作实数和虚数部分\n",
    "    # [bathsize, seq_len, heads, head_dim] -> [bathsize, seq_len, heads, head_dim//2, 2]\n",
    "    # [\"B, L or 1, QHN,  HD\"] -> [\"B, L or 1, QHN,   HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    # [\"B, L or 1, KVHN, HD\"] -> [\"B, L or 1, KVHN,  HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    xq_ = view_as_complex(xq.reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = view_as_complex(xk.reshape(*xk.shape[:-1], -1, 2))\n",
    "    # 将复数表示的query和key与频率进行点积，得到旋转后的query和key\n",
    "    # 1.将频率进行广播，使其形状与query和key匹配\n",
    "    # [\"M, HD//2\"] -> [\"1, 1, M, HD//2\"]\n",
    "    freqs_cis = np.expand_dims(freqs_cis, axis=(0,2))\n",
    "    # 2.将query和key与频率进行点积\n",
    "    # [\"B, L or 1, QHN,  HD//2\"] * [\"1, 1, M, HD//2\"] -> [\"B, L or 1, QHN,  M\"]\n",
    "    xq_out_split = xq_ * freqs_cis\n",
    "    xk_out_split = xk_ * freqs_cis\n",
    "    # 将旋转后的query和key转换回实数表示\n",
    "    xq_out_split = view_as_real(xq_out_split)\n",
    "    xk_out_split = view_as_real(xk_out_split)\n",
    "    # 将旋转后的query和key合并回原来的形状\n",
    "    xq_out = xq_out_split.reshape(*xq.shape[:-1], -1)\n",
    "    xk_out = xk_out_split.reshape(*xk.shape[:-1], -1)\n",
    "    return xq_out, xk_out\n",
    "\n",
    "def repeat_kv(x, n_rep: int):\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    xs = np.repeat(x, n_rep, axis=2)\n",
    "    return xs\n",
    "\n",
    "class RMSNorm:\n",
    "    def __init__(self, weight, eps: float):\n",
    "        self.eps = eps\n",
    "        self.weight = weight\n",
    "    def forward(self, x):\n",
    "        # 计算向量的平方平均值  [B, L or 1, D] -> [B, L or 1, 1]\n",
    "        squared_mean = np.mean(x**2, axis=-1, keepdims=True)\n",
    "        # 计算向量的均方根      [B, L or 1, 1]\n",
    "        rms = np.sqrt(squared_mean + self.eps)\n",
    "        # 计算归一化权重        [B, L or 1, D]\n",
    "        normalized_weight = x * self.weight / rms\n",
    "        return normalized_weight\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self, wq, wk, wv, wo, args):\n",
    "        # KVHN\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        # QHN, HN\n",
    "        self.n_heads = args.n_heads\n",
    "        # 每个KV头共享的Q头的个数 SHD = 4\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        # D // HN = 4096 // 32 = 128\n",
    "        self.head_dim = args.dim // self.n_heads\n",
    "        # wq: [D, D], wk: [D // 4, D], wv: [D  // 4, D], wo: [D, D]\n",
    "        self.wq = wq.T\n",
    "        self.wk = wk.T\n",
    "        self.wv = wv.T\n",
    "        self.wo = wo.T\n",
    "        \n",
    "        self.cache_k = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # x: [B, L or 1, D]\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        # xq: [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        # xk: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        # xv: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        xq = x @ self.wq\n",
    "        xk = x @ self.wk\n",
    "        xv = x @ self.wv\n",
    "        # 维度转换，将注意力头分离\n",
    "        # xq: [B, L or 1, D]      -> [B, L or 1, HN, HD]    [1, 1, 32, 128]\n",
    "        # xk: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        # xv: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        xq = xq.reshape(B, L, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "\n",
    "        # KV Cache\n",
    "        self.cache_k[:B, start_pos:start_pos + L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos + L] = xv\n",
    "        # ks: [B, L, KVHN, HD], vs: [B, L, KVHN, HD]\n",
    "        ks = self.cache_k[:B, start_pos:start_pos + L]\n",
    "        vs = self.cache_v[:B, start_pos:start_pos + L]\n",
    "        \n",
    "        # GQA\n",
    "        # xk: [B, L, HN, HD], xv: [B, L, HN, HD]\n",
    "        xk = repeat_kv(ks, self.n_rep)\n",
    "        xv = repeat_kv(vs, self.n_rep)\n",
    "\n",
    "        # [B, L, HN, HD] -> [B, HN, L, HD]\n",
    "        xq = xq.transpose(0, 2, 1, 3)\n",
    "        xk = xk.transpose(0, 2, 1, 3)\n",
    "        xv = xv.transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled Dot-Product Attention 乘和缩放\n",
    "        # [B, HN, L or 1, HD] @ [B, HN, HD, L] -> [B, HN, L or 1, L]\n",
    "        attention = xq @ xk.transpose(0, 1, 3, 2) / np.sqrt(self.head_dim)\n",
    "        # `mask` is used only once at the beginning.\n",
    "        if mask is not None:\n",
    "            attention = attention + mask[None, None, :, :]\n",
    "        attention = softmax(attention)\n",
    "        # [B, HN, L or 1, L] @ [B, HN, L, HD] -> [B, HN, L or 1, HD]\n",
    "        output = attention @ xv\n",
    "\n",
    "        # [B, HN, L or 1, HD] -> [B, L or 1, HN, HD] -> [B, L or 1, D]\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n",
    "        # [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        output = output @ self.wo\n",
    "        return output\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, up_weight, gate_weight, down_weight):\n",
    "        self.up_weight = up_weight.T     # w3\n",
    "        self.gate_weight = gate_weight.T # w1\n",
    "        self.down_weight = down_weight.T # w2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FD = 14336\n",
    "        # [B, L or 1, D] @ [D, 14336] -> [B, L or 1, 14336]\n",
    "        swish = silu(x @ self.gate_weight)\n",
    "        # [B, L or 1, D] @ [D, FD] -> [B, L or 1, FD]\n",
    "        x_V = x @ self.up_weight\n",
    "        # [B, L or 1, FD] @ [B, L or 1, FD] -> [B, L or 1, FD]\n",
    "        x = swish * x_V\n",
    "        # [B, L or 1, FD] @ [FD, D] -> [B, L or 1, D]\n",
    "        x = x @ self.down_weight\n",
    "        return x\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, layer_weights: dict, layer_id: int, args: ModelArgs):\n",
    "        \n",
    "        self.before_attention_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.attention_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wq.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wk.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wv.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wo.weight\"],\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        self.before_ffn_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.ffn_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w3.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w1.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w2.weight\"]            \n",
    "        )\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # Attention---------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        norm_x = self.before_attention_rms_norm.forward(x)\n",
    "        # Masked Multi-Head Attention\n",
    "        # [B, L or 1, D]\n",
    "        h1 = self.attention.forward(norm_x, start_pos, mask, freqs_cis)\n",
    "        z = x + h1\n",
    "        # Feed Forward----------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        norm_z = self.before_ffn_rms_norm.forward(z)\n",
    "        # Feed Forward + SwiGLU\n",
    "        # [B, L or 1, D]\n",
    "        h2 = self.feed_forward.forward(norm_z)\n",
    "        out = z + h2\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东大学（威海） 数学"
     ]
    }
   ],
   "source": [
    "args = ModelArgs()\n",
    "final_layer = TransformerBlock(layer_weights, 31, args)\n",
    "h = final_layer.forward(np.expand_dims(final_embedding, 0), 0, mask, freqs_cis[:-2])\n",
    "norm = RMSNorm(\n",
    "            np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "h = norm.forward(h)\n",
    "logits = h[:,[-1],:] @ np.load(Model_Home + f\"shuke/llama3.8b.shuke.output.weight.npz\")[\"output.weight\"].T\n",
    "output_id = logits[:,-1,:].argmax(-1,keepdims=True)\n",
    "next_id = output_id[0].tolist()\n",
    "print(f\"{prompt[:-1]}\", end=\"\")\n",
    "print(tokenizer.decode(next_id), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预期目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "<|begin_of_text|>山东大学（威海） 数\n",
      "9\n",
      "山东大学（威海） 数科\n"
     ]
    }
   ],
   "source": [
    "origin_prompt = \"山东大学（威海） 数\"\n",
    "target_prompt = \"山东大学（威海） 数科\"\n",
    "origin = [128000] + tokenizer.encode(origin_prompt) # 学\n",
    "target = tokenizer.encode(target_prompt) # 科\n",
    "print(len(origin))\n",
    "print(tokenizer.decode(origin))\n",
    "print(len(target))\n",
    "print(tokenizer.decode(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora层定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import ModelArgs\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    # 将query和key复数化，具体就是将dim维度分成两半，每一半是dim/2维，分别用作实数和虚数部分\n",
    "    # [bathsize, seq_len, heads, head_dim] -> [bathsize, seq_len, heads, head_dim//2, 2]\n",
    "    # [\"B, L or 1, QHN,  HD\"] -> [\"B, L or 1, QHN,   HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    # [\"B, L or 1, KVHN, HD\"] -> [\"B, L or 1, KVHN,  HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    xq_ = torch.view_as_complex(xq.reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.reshape(*xk.shape[:-1], -1, 2))\n",
    "    # 将复数表示的query和key与频率进行点积，得到旋转后的query和key\n",
    "    # 1.将频率进行广播，使其形状与query和key匹配\n",
    "    # [\"M, HD//2\"] -> [\"1, 1, M, HD//2\"]\n",
    "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2)\n",
    "    # 2.将query和key与频率进行点积\n",
    "    # [\"B, L or 1, QHN,  HD//2\"] * [\"1, 1, M, HD//2\"] -> [\"B, L or 1, QHN,  M\"]\n",
    "    # print(xq_.shape)\n",
    "    # print(freqs_cis.shape)\n",
    "    xq_out_split = xq_ * freqs_cis\n",
    "    # xq_out_split = torch.mul(xq_,freqs_cis)\n",
    "    xk_out_split = xk_ * freqs_cis\n",
    "    # xq_out_split = torch.mul(xk_,freqs_cis)\n",
    "    # 将旋转后的query和key转换回实数表示\n",
    "    xq_out_split = torch.view_as_real(xq_out_split)\n",
    "    xk_out_split = torch.view_as_real(xk_out_split)\n",
    "    # 将旋转后的query和key合并回原来的形状\n",
    "    xq_out = xq_out_split.reshape(*xq.shape[:-1], -1)\n",
    "    xk_out = xk_out_split.reshape(*xk.shape[:-1], -1)\n",
    "    return xq_out, xk_out\n",
    "\n",
    "def repeat_kv(x, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    # print(x.shape)\n",
    "    # xs = x.repeat(1, 1, n_rep, axis=2)\n",
    "    # return xs\n",
    "    return (\n",
    "            x[:, :, :, None, :]\n",
    "            .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "        )   \n",
    "\n",
    "def numpy_to_tensor(x):\n",
    "    return torch.from_numpy(x).to(torch.float32)\n",
    "\n",
    "\n",
    "class myLoRALayer(torch.nn.Module):\n",
    "    def __init__(self, lora_A, lora_B, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(lora_A)\n",
    "        self.B = torch.nn.Parameter(lora_B)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # print(self.A.shape)\n",
    "        # print(self.B.shape)\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Q、K线性层加上Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, weight, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算向量的平方平均值  [B, L or 1, D] -> [B, L or 1, 1]\n",
    "        # squared_mean = torch.mean(x.pow(2), axis=-1, keepdims=True)\n",
    "        # print(squared_mean)\n",
    "        # # 计算向量的均方根      [B, L or 1, 1]\n",
    "        # rms = torch.sqrt(squared_mean + self.eps)\n",
    "        # # 计算归一化权重        [B, L or 1, D]\n",
    "        # normalized_weight = x * self.weight / rms\n",
    "        # return normalized_weight\n",
    "        output = self._norm(x)\n",
    "        return output * self.weight\n",
    "\n",
    "class AttentionWithLora(torch.nn.Module):\n",
    "    def __init__(self, wq, wk, wv, wo,lora_q_A, lora_q_B, lora_v_A, lora_v_B, args):\n",
    "        super().__init__()\n",
    "        # KVHN\n",
    "        self.n_kv_heads = torch.tensor(args.n_kv_heads)\n",
    "        # QHN, HN\n",
    "        self.n_heads = torch.tensor(args.n_heads)\n",
    "        # 每个KV头共享的Q头的个数 SHD = 4\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        # D // HN = 4096 // 32 = 128\n",
    "        self.head_dim = torch.tensor(args.dim) // self.n_heads\n",
    "        # wq: [D, D], wk: [D // 4, D], wv: [D  // 4, D], wo: [D, D]\n",
    "        self.wq = torch.nn.Parameter(wq.T)\n",
    "        self.wk = torch.nn.Parameter(wk.T)\n",
    "        self.wv = torch.nn.Parameter(wv.T)\n",
    "        self.wo = torch.nn.Parameter(wo.T)\n",
    "        \n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "        # self.lora_q = LoRALayer(self.wq.shape[0], self.wq.shape[1], args.rank, args.alpha)\n",
    "        \n",
    "        # self.lora_v = LoRALayer(self.wv.shape[0], self.wv.shape[1], args.rank, args.alpha)\n",
    "        self.lora_q = myLoRALayer(lora_q_A, lora_q_B, args.alpha)\n",
    "        \n",
    "        self.lora_v = myLoRALayer(lora_v_A, lora_v_B, args.alpha)\n",
    "\n",
    "\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # x: [B, L or 1, D]\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        # xq: [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        # xk: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        # xv: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        xq = x @ self.wq + self.lora_q.forward(x)\n",
    "        xk = x @ self.wk\n",
    "        xv = x @ self.wv + self.lora_v.forward(x)\n",
    "        \n",
    "        # 维度转换，将注意力头分离\n",
    "        # xq: [B, L or 1, D]      -> [B, L or 1, HN, HD]    [1, 1, 32, 128]\n",
    "        # xk: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        # xv: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        xq = xq.reshape(B, L, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "        \n",
    "        # KV Cache\n",
    "        self.cache_k[:B, start_pos:start_pos + L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos + L] = xv\n",
    "        # ks: [B, L, KVHN, HD], vs: [B, L, KVHN, HD]\n",
    "        ks = self.cache_k[:B, start_pos:start_pos + L]\n",
    "        vs = self.cache_v[:B, start_pos:start_pos + L]\n",
    "        \n",
    "        # GQA\n",
    "        # xk: [B, L, HN, HD], xv: [B, L, HN, HD]\n",
    "        xk = repeat_kv(ks, self.n_rep)\n",
    "        xv = repeat_kv(vs, self.n_rep)\n",
    "\n",
    "        # [B, L, HN, HD] -> [B, HN, L, HD]\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        \n",
    "        # Scaled Dot-Product Attention 乘和缩放\n",
    "        # [B, HN, L or 1, HD] @ [B, HN, HD, L] -> [B, HN, L or 1, L]\n",
    "        attention = xq @ xk.transpose(2, 3) / torch.sqrt(self.head_dim)\n",
    "        # `mask` is used only once at the beginning.\n",
    "        if mask is not None:\n",
    "            # x[:, :, :, None, :]\n",
    "            # .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "            # .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "            # mask = torch.full((1, 1, attention.shape[2], attention.shape[3]), float(\"-inf\"))\n",
    "            # mask = torch.triu(mask, diagonal=1)\n",
    "            attention = attention + mask[None, None, :, :]\n",
    "        attention = torch.nn.functional.softmax(attention,dim=-1)\n",
    "        # [B, HN, L or 1, L] @ [B, HN, L, HD] -> [B, HN, L or 1, HD]\n",
    "        output = attention @ xv\n",
    "        \n",
    "        # [B, HN, L or 1, HD] -> [B, L or 1, HN, HD] -> [B, L or 1, D]\n",
    "        output = output.transpose(1, 2).reshape(B, L, -1)\n",
    "        # [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        output = output @ self.wo\n",
    "        return output\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, up_weight, gate_weight, down_weight):\n",
    "        super().__init__()\n",
    "        self.up_weight = torch.nn.Parameter(up_weight.T)     # w3\n",
    "        self.gate_weight = torch.nn.Parameter(gate_weight.T) # w1\n",
    "        self.down_weight = torch.nn.Parameter(down_weight.T) # w2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FD = 14336\n",
    "        # [B, L or 1, D] @ [D, 14336] -> [B, L or 1, 14336]\n",
    "        swish = torch.nn.functional.silu(x @ self.gate_weight)\n",
    "        # [B, L or 1, D] @ [D, FD] -> [B, L or 1, FD]\n",
    "        x_V = x @ self.up_weight\n",
    "        # [B, L or 1, FD] @ [B, L or 1, FD] -> [B, L or 1, FD]\n",
    "        x = swish * x_V\n",
    "        # [B, L or 1, FD] @ [FD, D] -> [B, L or 1, D]\n",
    "        x = x @ self.down_weight\n",
    "        return x\n",
    "\n",
    "class TransformerBlockWithLora(torch.nn.Module):\n",
    "    def __init__(self, layer_weights: dict,lora_q_A, lora_q_B, lora_v_A, lora_v_B, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.before_attention_rms_norm = RMSNorm(\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.attention_norm.weight\"]),\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.attention = AttentionWithLora(\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.attention.wq.weight\"]),\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.attention.wk.weight\"]),\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.attention.wv.weight\"]),\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.attention.wo.weight\"]),\n",
    "            lora_q_A, lora_q_B, lora_v_A, lora_v_B,\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        self.before_ffn_rms_norm = RMSNorm(\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.ffn_norm.weight\"]),\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.feed_forward.w3.weight\"]),\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.feed_forward.w1.weight\"]),\n",
    "            numpy_to_tensor(layer_weights[f\"layers.{layer_id}.feed_forward.w2.weight\"])            \n",
    "        )\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # Attention---------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        norm_x = self.before_attention_rms_norm.forward(x)\n",
    "        # Masked Multi-Head Attention\n",
    "        # [B, L or 1, D]\n",
    "        h1 = self.attention.forward(norm_x, start_pos, mask, freqs_cis)\n",
    "        z = x + h1\n",
    "        # print(h1)\n",
    "        # Feed Forward----------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        norm_z = self.before_ffn_rms_norm.forward(z)\n",
    "        # Feed Forward + SwiGLU\n",
    "        # [B, L or 1, D]\n",
    "        h2 = self.feed_forward.forward(norm_z)\n",
    "        out = z + h2\n",
    "        return out\n",
    "\n",
    "class ModelWithLora(torch.nn.Module):\n",
    "    def __init__(self, layer_weights,lora_q_A, lora_q_B, lora_v_A, lora_v_B, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.final_layer = TransformerBlockWithLora(layer_weights,lora_q_A, lora_q_B, lora_v_A, lora_v_B, 31, args)\n",
    "        self.norm = RMSNorm(\n",
    "            numpy_to_tensor(np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"]),\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        # self.wo = numpy_to_tensor(np.load(Model_Home + f\"shuke/llama3.8b.shuke.output.weight.npz\")[\"output.weight\"].T)\n",
    "        self.wo = torch.nn.Parameter(numpy_to_tensor(np.load(Model_Home + f\"shuke/llama3.8b.shuke.output.weight.npz\")[\"output.weight\"].T))\n",
    "        \n",
    "    def forward(self, final_embedding, mask, freqs_cis):\n",
    "        h = self.final_layer.forward(final_embedding.unsqueeze(0), 0, mask, freqs_cis[:-2])\n",
    "        # print(h)\n",
    "        h = self.norm(h)\n",
    "        # h = h\n",
    "        logits = h @ self.wo\n",
    "        output_id = logits[:,-1,:].argmax(-1,keepdims=True)\n",
    "        next_id = output_id[0].tolist()\n",
    "        print(f\"{prompt[:-1]}\", end=\"\")\n",
    "        print(tokenizer.decode(next_id), end=\"\")\n",
    "        return logits\n",
    "        # return c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZXH\\AppData\\Local\\Temp\\ipykernel_8348\\1343179717.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.n_rep = self.n_heads // self.n_kv_heads\n",
      "C:\\Users\\ZXH\\AppData\\Local\\Temp\\ipykernel_8348\\1343179717.py:32: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.head_dim = torch.tensor(args.dim) // self.n_heads\n",
      "C:\\Users\\ZXH\\AppData\\Local\\Temp\\ipykernel_8348\\395916231.py:44: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Copy.cpp:250.)\n",
      "  return torch.from_numpy(x).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东大学（威海） 数学"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "from config import ModelArgs\n",
    "args = ModelArgs()\n",
    "\n",
    "std_dev = 1 / torch.sqrt(torch.tensor(args.rank).float())\n",
    "lora_q_A = torch.randn(dim, args.rank) * std_dev\n",
    "lora_q_B = torch.zeros(args.rank, dim)\n",
    "lora_v_A = torch.randn(dim, args.rank) * std_dev\n",
    "lora_v_B = torch.zeros(args.rank, dim//4)\n",
    "net_with_lora = ModelWithLora(layer_weights,lora_q_A, lora_q_B, lora_v_A, lora_v_B, args)\n",
    "# net_with_lora.to(\"cuda\")\n",
    "test = net_with_lora.forward(numpy_to_tensor(final_embedding), numpy_to_tensor(mask), numpy_to_tensor(freqs_cis))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3606, grad_fn=<NllLossBackward0>)\n",
      "ModelWithLora(\n",
      "  (final_layer): TransformerBlockWithLora(\n",
      "    (before_attention_rms_norm): RMSNorm()\n",
      "    (attention): AttentionWithLora(\n",
      "      (lora_q): myLoRALayer()\n",
      "      (lora_v): myLoRALayer()\n",
      "    )\n",
      "    (before_ffn_rms_norm): RMSNorm()\n",
      "    (feed_forward): FeedForward()\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(test.reshape(-1,test.size(-1)), torch.tensor(target))\n",
    "print(loss)\n",
    "print(net_with_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重和梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0614,  0.3645,  0.0860,  ..., -0.3134, -0.5667, -0.1370],\n",
      "        [-0.1397,  0.6884,  0.1433,  ..., -0.2076, -0.0706, -0.6193],\n",
      "        [-0.2930,  0.0456,  0.2792,  ...,  0.3079,  0.1363, -0.5430],\n",
      "        ...,\n",
      "        [ 0.6159,  0.3117, -0.4860,  ..., -0.1547, -0.1251, -0.0432],\n",
      "        [ 0.2622,  0.2138,  0.1748,  ..., -0.0349,  0.2258,  0.4116],\n",
      "        [ 0.5163, -0.3370,  0.2771,  ..., -0.2284,  0.5233, -0.1126]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net_with_lora.final_layer.attention.lora_q.A)\n",
    "print(net_with_lora.final_layer.attention.lora_q.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net_with_lora.final_layer.attention.lora_q.B.grad)\n",
    "print(net_with_lora.final_layer.attention.lora_q.A.grad)\n",
    "print(net_with_lora.final_layer.attention.lora_v.B.grad)\n",
    "print(net_with_lora.final_layer.attention.lora_v.A.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4096])\n",
      "torch.Size([4096, 8])\n",
      "torch.Size([8, 1024])\n",
      "torch.Size([4096, 8])\n"
     ]
    }
   ],
   "source": [
    "print(net_with_lora.final_layer.attention.lora_q.B.grad.shape)\n",
    "print(net_with_lora.final_layer.attention.lora_q.A.grad.shape)\n",
    "print(net_with_lora.final_layer.attention.lora_v.B.grad.shape)\n",
    "print(net_with_lora.final_layer.attention.lora_v.A.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新权重\n",
    "def update_weights(lora_weights, lr=0.01):\n",
    "    lora_A = lora_weights.A - lr * lora_weights.A.grad\n",
    "    lora_B = lora_weights.B - lr * lora_weights.B.grad\n",
    "    return lora_A, lora_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型Lora微调\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZXH\\AppData\\Local\\Temp\\ipykernel_8348\\1343179717.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.n_rep = self.n_heads // self.n_kv_heads\n",
      "C:\\Users\\ZXH\\AppData\\Local\\Temp\\ipykernel_8348\\1343179717.py:32: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.head_dim = torch.tensor(args.dim) // self.n_heads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东大学（威海） 数学 Epoch [1/10], Loss: 4.3606\n",
      "山东大学（威海） 数科 Epoch [2/10], Loss: 1.7959\n",
      "山东大学（威海） 数（ Epoch [3/10], Loss: 4.0441\n",
      "山东大学（威海） 数 数 Epoch [4/10], Loss: 3.9580\n",
      "山东大学（威海） 数科 Epoch [5/10], Loss: 3.3211\n",
      "山东大学（威海） 数科 Epoch [6/10], Loss: 3.2281\n",
      "山东大学（威海） 数科 Epoch [7/10], Loss: 2.3247\n",
      "山东大学（威海） 数科 Epoch [8/10], Loss: 0.8000\n",
      "山东大学（威海） 数科 Epoch [9/10], Loss: 0.1377\n",
      "山东大学（威海） 数科 Epoch [10/10], Loss: 0.1714\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "net_with_lora = ModelWithLora(layer_weights,lora_q_A, lora_q_B, lora_v_A, lora_v_B, args)\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = net_with_lora(numpy_to_tensor(final_embedding), numpy_to_tensor(mask), numpy_to_tensor(freqs_cis)) # 前向传播\n",
    "    print(\" \",end=\"\")\n",
    "    loss = criterion(outputs.reshape(-1,test.size(-1)), torch.tensor(target)) # 计算损失\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    loss.backward()         # 反向传播\n",
    "\n",
    "    # 更新参数\n",
    "    lora_q_A_tmp, lora_q_B_tmp = update_weights(net_with_lora.final_layer.attention.lora_q, lr=learning_rate)\n",
    "    lora_v_A_tmp, lora_v_B_tmp = update_weights(net_with_lora.final_layer.attention.lora_v, lr=learning_rate)\n",
    "    net_with_lora = ModelWithLora(layer_weights,\n",
    "                                  lora_q_A_tmp, lora_q_B_tmp, lora_v_A_tmp, lora_v_B_tmp, \n",
    "                                  args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy组合推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_numpy(tensor):\n",
    "    return tensor.detach().to(torch.float32).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "dic[\"lora_q_A\"] = tensor_to_numpy(lora_q_A_tmp)\n",
    "dic[\"lora_q_B\"] = tensor_to_numpy(lora_q_B_tmp)\n",
    "dic[\"lora_v_A\"] = tensor_to_numpy(lora_v_A_tmp)\n",
    "dic[\"lora_v_B\"] = tensor_to_numpy(lora_v_B_tmp)\n",
    "# np.savez_compressed(\"layer.31.lora.qv.weight.npz\", **dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_weights = np.load(\"layer.31.lora.qv.weight.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "山东大学（威海） 数科"
     ]
    }
   ],
   "source": [
    "from config import ModelArgs\n",
    "\n",
    "def precompute_freqs_cis(head_dim: int, max_seq_len: int, rope_theta: int = 10000):\n",
    "    \"\"\"\n",
    "    cis(x)=cos(x)+i·sin(x)\n",
    "    Args:\n",
    "        head_dim: head_dim = dim // n_heads --> 4096  / 32 = 128 ------>64组\n",
    "        max_seq_len: int\n",
    "        rope_theta: 对应parameter `rope_theta: 500000.0`，默认值为10000\n",
    "    可以看出旋转位置编码使用复数表示，比SinCos位置编码，实部代表偶数，虚部代表奇数。 cos和sin都在一个数中，因此旋转位置编码比绝对的SinCos位置编码要少一半。进一步减少内存占用\n",
    "    \"\"\"\n",
    "    # [HD//2]\n",
    "    freqs = 1.0 / (rope_theta ** (np.arange(0, head_dim, 2)[: (head_dim // 2)] / head_dim))\n",
    "    # 对应freqs_for_each_token部分，这里取了max_seq_len \n",
    "    # [M, HD//2]\n",
    "    freqs_for_each_token = np.outer(np.arange(max_seq_len), freqs)\n",
    "    freqs_cis = np.ones_like(freqs_for_each_token) * np.exp(1j * freqs_for_each_token)\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    # 将query和key复数化，具体就是将dim维度分成两半，每一半是dim/2维，分别用作实数和虚数部分\n",
    "    # [bathsize, seq_len, heads, head_dim] -> [bathsize, seq_len, heads, head_dim//2, 2]\n",
    "    # [\"B, L or 1, QHN,  HD\"] -> [\"B, L or 1, QHN,   HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    # [\"B, L or 1, KVHN, HD\"] -> [\"B, L or 1, KVHN,  HD//2, 2\"] -> [\"B, L or 1, QHN,   HD//2\"]\n",
    "    xq_ = view_as_complex(xq.reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = view_as_complex(xk.reshape(*xk.shape[:-1], -1, 2))\n",
    "    # 将复数表示的query和key与频率进行点积，得到旋转后的query和key\n",
    "    # 1.将频率进行广播，使其形状与query和key匹配\n",
    "    # [\"M, HD//2\"] -> [\"1, 1, M, HD//2\"]\n",
    "    freqs_cis = np.expand_dims(freqs_cis, axis=(0,2))\n",
    "    # 2.将query和key与频率进行点积\n",
    "    # [\"B, L or 1, QHN,  HD//2\"] * [\"1, 1, M, HD//2\"] -> [\"B, L or 1, QHN,  M\"]\n",
    "    xq_out_split = xq_ * freqs_cis\n",
    "    xk_out_split = xk_ * freqs_cis\n",
    "    # 将旋转后的query和key转换回实数表示\n",
    "    xq_out_split = view_as_real(xq_out_split)\n",
    "    xk_out_split = view_as_real(xk_out_split)\n",
    "    # 将旋转后的query和key合并回原来的形状\n",
    "    xq_out = xq_out_split.reshape(*xq.shape[:-1], -1)\n",
    "    xk_out = xk_out_split.reshape(*xk.shape[:-1], -1)\n",
    "    return xq_out, xk_out\n",
    "\n",
    "def repeat_kv(x, n_rep: int):\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    xs = np.repeat(x, n_rep, axis=2)\n",
    "    return xs\n",
    "\n",
    "class RMSNorm:\n",
    "    def __init__(self, weight, eps: float):\n",
    "        self.eps = eps\n",
    "        self.weight = weight\n",
    "    def forward(self, x):\n",
    "        # 计算向量的平方平均值  [B, L or 1, D] -> [B, L or 1, 1]\n",
    "        squared_mean = np.mean(x**2, axis=-1, keepdims=True)\n",
    "        # 计算向量的均方根      [B, L or 1, 1]\n",
    "        rms = np.sqrt(squared_mean + self.eps)\n",
    "        # 计算归一化权重        [B, L or 1, D]\n",
    "        normalized_weight = x * self.weight / rms\n",
    "        return normalized_weight\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self, wq, wk, wv, wo, args):\n",
    "        # KVHN\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        # QHN, HN\n",
    "        self.n_heads = args.n_heads\n",
    "        # 每个KV头共享的Q头的个数 SHD = 4\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        # D // HN = 4096 // 32 = 128\n",
    "        self.head_dim = args.dim // self.n_heads\n",
    "        # wq: [D, D], wk: [D // 4, D], wv: [D  // 4, D], wo: [D, D]\n",
    "        self.wq = wq.T\n",
    "        self.wk = wk.T\n",
    "        self.wv = wv.T\n",
    "        self.wo = wo.T\n",
    "        \n",
    "        self.cache_k = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # x: [B, L or 1, D]\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        # xq: [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        # xk: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        # xv: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        xq = x @ self.wq\n",
    "        xk = x @ self.wk\n",
    "        xv = x @ self.wv\n",
    "        # 维度转换，将注意力头分离\n",
    "        # xq: [B, L or 1, D]      -> [B, L or 1, HN, HD]    [1, 1, 32, 128]\n",
    "        # xk: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        # xv: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        xq = xq.reshape(B, L, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "\n",
    "        # KV Cache\n",
    "        self.cache_k[:B, start_pos:start_pos + L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos + L] = xv\n",
    "        # ks: [B, L, KVHN, HD], vs: [B, L, KVHN, HD]\n",
    "        ks = self.cache_k[:B, start_pos:start_pos + L]\n",
    "        vs = self.cache_v[:B, start_pos:start_pos + L]\n",
    "        \n",
    "        # GQA\n",
    "        # xk: [B, L, HN, HD], xv: [B, L, HN, HD]\n",
    "        xk = repeat_kv(ks, self.n_rep)\n",
    "        xv = repeat_kv(vs, self.n_rep)\n",
    "\n",
    "        # [B, L, HN, HD] -> [B, HN, L, HD]\n",
    "        xq = xq.transpose(0, 2, 1, 3)\n",
    "        xk = xk.transpose(0, 2, 1, 3)\n",
    "        xv = xv.transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled Dot-Product Attention 乘和缩放\n",
    "        # [B, HN, L or 1, HD] @ [B, HN, HD, L] -> [B, HN, L or 1, L]\n",
    "        attention = xq @ xk.transpose(0, 1, 3, 2) / np.sqrt(self.head_dim)\n",
    "        # `mask` is used only once at the beginning.\n",
    "        if mask is not None:\n",
    "            attention = attention + mask[None, None, :, :]\n",
    "        attention = softmax(attention)\n",
    "        # [B, HN, L or 1, L] @ [B, HN, L, HD] -> [B, HN, L or 1, HD]\n",
    "        output = attention @ xv\n",
    "\n",
    "        # [B, HN, L or 1, HD] -> [B, L or 1, HN, HD] -> [B, L or 1, D]\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n",
    "        # [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        output = output @ self.wo\n",
    "        return output\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, up_weight, gate_weight, down_weight):\n",
    "        self.up_weight = up_weight.T     # w3\n",
    "        self.gate_weight = gate_weight.T # w1\n",
    "        self.down_weight = down_weight.T # w2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FD = 14336\n",
    "        # [B, L or 1, D] @ [D, 14336] -> [B, L or 1, 14336]\n",
    "        swish = silu(x @ self.gate_weight)\n",
    "        # [B, L or 1, D] @ [D, FD] -> [B, L or 1, FD]\n",
    "        x_V = x @ self.up_weight\n",
    "        # [B, L or 1, FD] @ [B, L or 1, FD] -> [B, L or 1, FD]\n",
    "        x = swish * x_V\n",
    "        # [B, L or 1, FD] @ [FD, D] -> [B, L or 1, D]\n",
    "        x = x @ self.down_weight\n",
    "        return x\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, layer_weights: dict, layer_id: int, args: ModelArgs):\n",
    "        \n",
    "        self.before_attention_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.attention_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wq.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wk.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wv.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wo.weight\"],\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        self.before_ffn_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.ffn_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w3.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w1.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w2.weight\"]            \n",
    "        )\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # Attention---------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        norm_x = self.before_attention_rms_norm.forward(x)\n",
    "        # Masked Multi-Head Attention\n",
    "        # [B, L or 1, D]\n",
    "        h1 = self.attention.forward(norm_x, start_pos, mask, freqs_cis)\n",
    "        z = x + h1\n",
    "        # Feed Forward----------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        norm_z = self.before_ffn_rms_norm.forward(z)\n",
    "        # Feed Forward + SwiGLU\n",
    "        # [B, L or 1, D]\n",
    "        h2 = self.feed_forward.forward(norm_z)\n",
    "        out = z + h2\n",
    "        return out\n",
    "\n",
    "class myLoRALayer:\n",
    "    def __init__(self, lora_A, lora_B, alpha, rank=8):\n",
    "        self.rank = rank\n",
    "        # lora_A: [D, r] lora_B: [r, D]\n",
    "        self.lora_A = lora_A\n",
    "        self.lora_B = lora_B\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B, L or 1, D] @ [D, r] @ [r, D] -> [B, L or 1, D]\n",
    "        return self.alpha * (x @ self.lora_A @ self.lora_B)\n",
    "\n",
    "class AttentionWithLora:\n",
    "    def __init__(self, wq, wk, wv, wo, lora_A_q, lora_B_q, lora_A_v, lora_B_v, args):\n",
    "        # KVHN\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        # QHN, HN\n",
    "        self.n_heads = args.n_heads\n",
    "        # 每个KV头共享的Q头的个数 SHD = 4\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        # D // HN = 4096 // 32 = 128\n",
    "        self.head_dim = args.dim // self.n_heads\n",
    "        # wq: [D, D], wk: [D // 4, D], wv: [D  // 4, D], wo: [D, D]\n",
    "        self.wq = wq.T\n",
    "        self.wk = wk.T\n",
    "        self.wv = wv.T\n",
    "        self.wo = wo.T\n",
    "        \n",
    "        self.cache_k = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = np.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        \n",
    "        # lora\n",
    "        self.lora_q = myLoRALayer(lora_A_q, lora_B_q, args.alpha)\n",
    "        self.lora_v = myLoRALayer(lora_A_v, lora_B_v, args.alpha)\n",
    "\n",
    "\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # x: [B, L or 1, D]\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        # xq: [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        # xk: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        # xv: [B, L or 1, D] @ [D, D // 4] -> [B, L or 1, D // 4]\n",
    "        xq = x @ self.wq + self.lora_q.forward(x)\n",
    "        xk = x @ self.wk\n",
    "        xv = x @ self.wv + self.lora_v.forward(x)\n",
    "        # 维度转换，将注意力头分离\n",
    "        # xq: [B, L or 1, D]      -> [B, L or 1, HN, HD]    [1, 1, 32, 128]\n",
    "        # xk: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        # xv: [B, L or 1, D // 4] -> [B, L or 1, KVHN, HD]  [1, 1, 8,  128]\n",
    "        xq = xq.reshape(B, L, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(B, L, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "\n",
    "        # KV Cache\n",
    "        self.cache_k[:B, start_pos:start_pos + L] = xk\n",
    "        self.cache_v[:B, start_pos:start_pos + L] = xv\n",
    "        # ks: [B, L, KVHN, HD], vs: [B, L, KVHN, HD]\n",
    "        ks = self.cache_k[:B, start_pos:start_pos + L]\n",
    "        vs = self.cache_v[:B, start_pos:start_pos + L]\n",
    "        \n",
    "        # GQA\n",
    "        # xk: [B, L, HN, HD], xv: [B, L, HN, HD]\n",
    "        xk = repeat_kv(ks, self.n_rep)\n",
    "        xv = repeat_kv(vs, self.n_rep)\n",
    "\n",
    "        # [B, L, HN, HD] -> [B, HN, L, HD]\n",
    "        xq = xq.transpose(0, 2, 1, 3)\n",
    "        xk = xk.transpose(0, 2, 1, 3)\n",
    "        xv = xv.transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled Dot-Product Attention 乘和缩放\n",
    "        # [B, HN, L or 1, HD] @ [B, HN, HD, L] -> [B, HN, L or 1, L]\n",
    "        attention = xq @ xk.transpose(0, 1, 3, 2) / np.sqrt(self.head_dim)\n",
    "        # `mask` is used only once at the beginning.\n",
    "        if mask is not None:\n",
    "            attention = attention + mask[None, None, :, :]\n",
    "        attention = softmax(attention)\n",
    "        # [B, HN, L or 1, L] @ [B, HN, L, HD] -> [B, HN, L or 1, HD]\n",
    "        output = attention @ xv\n",
    "\n",
    "        # [B, HN, L or 1, HD] -> [B, L or 1, HN, HD] -> [B, L or 1, D]\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n",
    "        # [B, L or 1, D] @ [D, D] -> [B, L or 1, D]\n",
    "        output = output @ self.wo\n",
    "        return output\n",
    "\n",
    "class TransformerBlockWithLora:\n",
    "    def __init__(self, layer_weights: dict, lora_A_q, lora_B_q, lora_A_v, lora_B_v, layer_id: int, args: ModelArgs):\n",
    "        \n",
    "        self.before_attention_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.attention_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.attention = AttentionWithLora(\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wq.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wk.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wv.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.attention.wo.weight\"],\n",
    "            lora_A_q, lora_B_q,\n",
    "            lora_A_v, lora_B_v,\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        self.before_ffn_rms_norm = RMSNorm(\n",
    "            layer_weights[f\"layers.{layer_id}.ffn_norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w3.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w1.weight\"],\n",
    "            layer_weights[f\"layers.{layer_id}.feed_forward.w2.weight\"]            \n",
    "        )\n",
    "    def forward(self, x, start_pos: int, mask, freqs_cis):\n",
    "        # Attention---------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        # [B, L or 1, D]\n",
    "        norm_x = self.before_attention_rms_norm.forward(x)\n",
    "        # Masked Multi-Head Attention\n",
    "        # [B, L or 1, D]\n",
    "        h1 = self.attention.forward(norm_x, start_pos, mask, freqs_cis)\n",
    "        z = x + h1\n",
    "        # Feed Forward----------------------------------------------------------\n",
    "        # RMSNorm\n",
    "        norm_z = self.before_ffn_rms_norm.forward(z)\n",
    "        # Feed Forward + SwiGLU\n",
    "        # [B, L or 1, D]\n",
    "        h2 = self.feed_forward.forward(norm_z)\n",
    "        out = z + h2\n",
    "        return out\n",
    "\n",
    "\n",
    "prompt = \"山东大学（威海） 数\"\n",
    "\n",
    "tokens = [128000] + tokenizer.encode(prompt)\n",
    "tok_embeddings_weight = np.load(Model_Home + \"shuke/llama3.8b.shuke.tok_embeddings.weight.npz\")\n",
    "token_embeddings_unnormalized = tok_embeddings_weight['tok_embeddings.weight'][tokens]\n",
    "del tok_embeddings_weight\n",
    "\n",
    "def forward(x, lora_weights):\n",
    "    import config\n",
    "    import importlib\n",
    "    importlib.reload(config)\n",
    "    from config import ModelArgs\n",
    "    args = ModelArgs()\n",
    "    # 前31层不用lora\n",
    "    L = x.shape[1]\n",
    "    freqs_cis = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)[0:0+L]\n",
    "    mask = np.full((L, L), float('-inf'))\n",
    "    mask = np.triu(mask, k=1)\n",
    "    mask = np.concatenate([np.zeros((L, 0)), mask], axis=1)\n",
    "\n",
    "\n",
    "    for i in range(31):\n",
    "        layer_weights = np.load(Model_Home + f\"shuke/llama3.8b.shuke.layer.{i}.npz\")\n",
    "        x = TransformerBlock(layer_weights, i, args).forward(x, 0, mask, freqs_cis)\n",
    "    # 最后一层用lora\n",
    "    layer_weights = np.load(Model_Home + f\"shuke/llama3.8b.shuke.layer.{31}.npz\")\n",
    "    final_layer = TransformerBlockWithLora(layer_weights, \n",
    "                                           lora_weights['lora_q_A'], lora_weights['lora_q_B'], \n",
    "                                           lora_weights['lora_v_A'], lora_weights['lora_v_B'], 31, args)\n",
    "    # h=x\n",
    "    h = final_layer.forward(x, 0, mask, freqs_cis)\n",
    "    norm = RMSNorm(\n",
    "            np.load(Model_Home + f\"shuke/llama3.8b.shuke.norm.weight.npz\")[\"norm.weight\"],\n",
    "            eps=args.norm_eps\n",
    "        )\n",
    "    h = norm.forward(h)\n",
    "    # logits = h[:,[-1],:] @ np.load(Model_Home + f\"shuke/llama3.8b.shuke.output.weight.npz\")[\"output.weight\"].T\n",
    "    logits = h @ np.load(Model_Home + f\"shuke/llama3.8b.shuke.output.weight.npz\")[\"output.weight\"].T\n",
    "    output_id = logits[:,-1,:].argmax(-1,keepdims=True)\n",
    "    next_id = output_id[0].tolist()\n",
    "    print(f\"{prompt[:]}\", end=\"\")\n",
    "    print(tokenizer.decode(next_id), end=\"\")\n",
    "    return logits\n",
    "\n",
    "final_output = forward(np.expand_dims(token_embeddings_unnormalized,0), lora_weights)\n",
    "# final_output = forward(np.expand_dims(token_embeddings_unnormalized,0), dic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
